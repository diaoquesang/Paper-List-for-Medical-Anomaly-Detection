<div id = "top"></div>

<div align="center">

[![](https://capsule-render.vercel.app/api?type=waving&height=200&color=0:2193b0,100:45ed3F&text=ğŸŒŸ%20Paper%20List%20for%20Medical%20Anomaly%20Detection&fontSize=30&fontAlignY=40&fontColor=FFFFFF)](#top)

</div>

<div align="center">
  

[![](https://img.shields.io/github/stars/diaoquesang/Paper-List-for-Medical-Anomaly-Detection)](https://github.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection)[![](https://img.shields.io/github/forks/diaoquesang/Paper-List-for-Medical-Anomaly-Detection)](https://github.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection)[![](https://img.shields.io/github/issues/diaoquesang/Paper-List-for-Medical-Anomaly-Detection)](https://github.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection/issues)[![](https://img.shields.io/github/license/diaoquesang/Paper-List-for-Medical-Anomaly-Detection)](https://github.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection/blob/main/LICENSE)[![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdiaoquesang%2FPaper-List-for-Medical-Anomaly-Detection&label=visitors&countColor=%2337d67a&style=flat&labelStyle=none)](https://visitorbadge.io/status?path=https%3A%2F%2Fgithub.com%2Fdiaoquesang%2FPaper-List-for-Medical-Anomaly-Detection)

</div>

**ğŸ¦‰ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…ÙŠÙ†:[y ifeed Sun (22 'HD U-it Mo Undergraduate)](https://diaoquesang.github.io/),[Jun Goodj IA (23 'HD U Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠØ©)](https://github.com/BeistMedAI),[Ø§Ù„Ø®Ø´Ø¨ (22 'i-immomomo](https://github.com/267588),[Zhang Chen (21 'HD U-It Mo undergraduate/25' S Master)](https://benny0323.github.io/bio/),[y u-heality Ù‡Ùˆ (23 '](https://github.com/Black0226),[Jinhongwang (21 'ZJ U PhD)](https://wang-jinhong.github.io/),[Jincheng L I (23 'NTU Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠØ©)](https://github.com/li00000011).**

**ğŸ“ Ø¯ÙŠØ¨ÙˆÙŠÙƒÙŠ:[ØªÙˆÙ„ÙŠØ¯ ÙˆØ«Ø§Ø¦Ù‚ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© github Ø¨Ù†Ù‚Ø±Ø© ÙˆØ§Ø­Ø¯Ø©](https://deepwiki.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection).**

**ğŸ“¦ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø£Ø®Ø±Ù‰:[1][Bone Suppression in Chest X-Rays: A Deep Survey](<https://github.com/diaoquesang/A-detailed-summarization-about-bone-suppression-in-Chest-X-rays>),[2][A Paper List for Prototypical Learning](<https://github.com/BeistMedAI/Paper-List-for-Prototypical-Learning>),[3][A Paper List for Cell Detection](<https://github.com/li00000011/Paper-List-for-Cell-Detection>),[4][Medical-AI-Guide](<https://github.com/diaoquesang/Medical-AI-Guide/>),[5][Paper List for Medical Reasoning Large Language Models](<https://github.com/HovChen/Paper-List-for-Medical-Reasoning-Large-Language-Models>).**

### Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ùƒ Ù„Ù„Ø§Ù†Ø¶Ù…Ø§Ù… Ø¥Ù„ÙŠÙ†Ø§ Ø¨Ø§Ù„Ø§ØªØµØ§Ù„:[szhsxhsyf@hdu.edu.cn](mailto:szhsxhsyf@hdu.edu.cn).

<div>
<img src="https://github.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection/blob/main/logos/HDU.png" height="45px" href="https://www.hdu.edu.cn/">
<img src="https://github.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection/blob/main/logos/ITMO.jpg" height="45px" href="https://en.itmo.ru/">
<img src="https://github.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection/blob/main/logos/SEU.jpg" height="45px" href="https://www.seu.edu.cn/">
<img src="https://github.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection/blob/main/logos/XDU.jpg" height="45px" href="https://www.xidian.edu.cn/">
<img src="https://github.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection/blob/main/logos/ZJU.png" height="45px" href="https://www.zju.edu.cn/">
<img src="https://github.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection/blob/main/logos/NTU.jpg" height="45px" href="https://www.ntu.edu.cn/">
<img src="https://github.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection/blob/main/logos/CUHK-SZ.png" height="45px" href="https://www.cuhk.edu.cn/zh-hans">
</div>

## ğŸ“‡ Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª

-   [**1. Ø­Ù„ "Ø±Ø³Ù… Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù‡ÙˆÙŠØ©"**](#s1)
-   [**2. Ø§Ù„ØªØ¹Ù„Ù… ØªØ­Øª Ø¥Ø´Ø±Ø§Ù**](#s2)
-   [**3. Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù**](#s3)
-   [**4. Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ AE**](#s)
-   [**5. Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ GAN**](#s5)
-   [**6. Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¯ÙÙ‚**](#s6)
-   [**7. Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø±**](#s7)
-   [**8. Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØµØ­ÙŠØ­**](#s8)
-   [**9. Ø§Ù„Ø§Ù†ØµÙ‡Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·**](#s9)
-   [**10. Ù†Ù…Ø§Ø°Ø¬ Ù„ØºØ© Ø§Ù„Ø±Ø¤ÙŠØ©**](#s10)
-   [**11. ØªÙ‚Ø·ÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙØ©**](#s11)
-   [**12. ØªØ¹Ù„Ù… Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·**](#s12)
-   [**13. Ø¬ÙŠÙ„ Ø§Ù„Ø´Ø°ÙˆØ°**](#s13)
-   [**14. Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±**](#s14)

## âœ Ø§Ù„Ù†ØµØ§Ø¦Ø­

-   \*: Ø£ÙˆØ±Ø§Ù‚ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ° ØºÙŠØ± Ø§Ù„Ø·Ø¨ÙŠ

-   : Octocat :: Code

## 1. Ø­Ù„ "Ø±Ø³Ù… Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù‡ÙˆÙŠØ©"<div id = "s1"></div>

-   [\[CV PR 2025\]](https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Dinomaly_The_Less_Is_More_Philosophy_in_Multi-Class_Unsupervised_Anomaly_CVPR_2025_paper.html)**Dinomaly: ÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ø§Ù„ÙÙ„Ø³ÙØ© Ø£Ù‚Ù„ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø°ÙˆØ° ØºÙŠØ± Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù**[: Octocat:](https://github.com/guojiajeremy/dinomaly)

    _Gu O ØŒ J Ia Ùˆ L U ØŒ Shuai Ùˆ Zhang ØŒ Weixing Ùˆ Chen ØŒ Fan Gan DL I ØŒ Huiqi Ùˆ Liao ØŒ Hon_

<div align="center">
  <img src="https://github.com/user-attachments/assets/1ef17bf6-41a6-41c6-8f5d-7361b98f4e81" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Recent studies highlighted a practical setting of unsupervised anomaly detection (UAD) that builds a unified model for multi-class images. Despite various advancements addressing this challenging task, the detection performance under the multi-class setting still lags far behind state-of-the-art class-separated models. Our research aims to bridge this substantial performance gap. In this paper, we present Dinomaly, a minimalist reconstruction-based anomaly detection framework that harnesses pure Transformer architectures without relying on complex designs, additional modules, or specialized tricks. Given this powerful framework consisting of only Attentions and MLPs, we found four simple components that are essential to multi-class anomaly detection: (1) Scalable foundation Transformers that extracts universal and discriminative features, (2) Noisy Bottleneck where pre-existing Dropouts do all the noise injection tricks, (3) Linear Attention that naturally cannot focus, and (4) Loose Reconstruction that does not force layer-to-layer and point-by-point reconstruction. Extensive experiments are conducted across popular anomaly detection benchmarks including MVTec-AD, VisA, and Real-IAD. Our proposed Dinomaly achieves impressive image-level AUROC of 99.6%, 98.7%, and 89.3% on the three datasets respectively, which is not only superior to state-of-the-art multi-class UAD methods, but also achieves the most advanced class-separated UAD records.
</details>

-   \*[\[Neurips 2022\]](https://proceedings.neurips.cc/paper_files/paper/2022/file/1d774c112926348c3e25ea47d87c835b-Paper-Conference.pdf)**Ù†Ù…ÙˆØ°Ø¬ Ù…ÙˆØ­Ø¯ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ° Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª**[: Octocat:](https://github.com/zhiyuanyou/uniad)

    _Ø£Ù†Øª ØŒ z hi unlement case dc ui ØŒ l eia nds you y u jun and yang ØŒ kai and l u ØŒ Î¾ nand z heng ØŒ yuan le ØŒ x in one_

<div align="center">
  <img src="https://github.com/user-attachments/assets/ab761cad-56ed-4b6d-8d6d-c333762b992c" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Despite the rapid advance of unsupervised anomaly detection, existing methods require to train separate models for different objects. In this work, we present UniAD that accomplishes anomaly detection for multiple classes with a unified framework. Under such a challenging setting, popular reconstruction networks may fall into an" identical shortcut", where both normal and anomalous samples can be well recovered, and hence fail to spot outliers. To tackle this obstacle, we make three improvements. First, we revisit the formulations of fully-connected layer, convolutional layer, as well as attention layer, and confirm the important role of query embedding (ie, within attention layer) in preventing the network from learning the shortcut. We therefore come up with a layer-wise query decoder to help model the multi-class distribution. Second, we employ a neighbor masked attention module to further avoid the information leak from the input feature to the reconstructed output feature. Third, we propose a feature jittering strategy that urges the model to recover the correct message even with noisy inputs. We evaluate our algorithm on MVTec-AD and CIFAR-10 datasets, where we surpass the state-of-the-art alternatives by a sufficiently large margin. For example, when learning a unified model for 15 categories in MVTec-AD, we surpass the second competitor on the tasks of both anomaly detection (from 88.1% to 96.5%) and anomaly localization (from 89.5% to 96.8%). Code is available at https://github.com/zhiyuanyou/UniAD.
</details>

## 2. Ø§Ù„ØªØ¹Ù„Ù… ØªØ­Øª Ø¥Ø´Ø±Ø§Ù<div id = "s2"></div>

-   \*[\[CV PR 2024\]](https://openaccess.thecvf.com/content/CVPR2024/html/Baitieva_Supervised_Anomaly_Detection_for_Complex_Industrial_Images_CVPR_2024_paper.html)**Ø§Ù„ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ© Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©**[: Octocat:](https://github.com/abc-125/segad)

    _Baitieva ØŒ Aimira Ùˆ Hurych ØŒ David and Besnier ØŒ Victor and Bernard ØŒ Olivier_

<div align="center">
  <img src="https://github.com/user-attachments/assets/5b29c1bb-765d-4bc3-9ddd-38bde1715e10" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Automating visual inspection in industrial production lines is essential for increasing product quality across various industries. Anomaly detection (AD) methods serve as robust tools for this purpose. However existing public datasets primarily consist of images without anomalies limiting the practical application of AD methods in production settings. To address this challenge we present (1) the Valeo Anomaly Dataset (VAD) a novel real-world industrial dataset comprising 5000 images including 2000 instances of challenging real defects across more than 20 subclasses. Acknowledging that traditional AD methods struggle with this dataset we introduce (2) Segmentation-based Anomaly Detector (SegAD). First SegAD leverages anomaly maps as well as segmentation maps to compute local statistics. Next SegAD uses these statistics and an optional supervised classifier score as input features for a Boosted Random Forest (BRF) classifier yielding the final anomaly score. Our SegAD achieves state-of-the-art performance on both VAD (+ 2.1% AUROC) and the VisA dataset (+ 0.4% AUROC). The code and the models are publicly available.
</details>

-   [\[CV PR 2019\]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Cascaded_Generative_and_Discriminative_Learning_for_Microcalcification_Detection_in_Breast_CVPR_2019_paper.pdf)**Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠ ÙˆØ§Ù„ØªÙ…ÙŠÙŠØ²ÙŠ Ø§Ù„Ù…ØªØªØ§Ù„ÙŠ Ù„Ù„ÙƒØ´Ù Ø¹Ù† microcalcific**

    _Zhang ØŒ Fan Ùˆ Luo ØŒ Lin Gan's Sun ØŒ Î¾ n position Ùˆ Zhou ØŒ z vey ØŒ x iu andy u ØŒ y i zhou and wang ØŒ y i axis_

<div align="center">
  <img src="https://github.com/user-attachments/assets/41569c42-7921-4602-9a35-cad58b06147e" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Accurate microcalcification (mC) detection is of great importance due to its high proportion in early breast cancers. Most of the previous mC detection methods belong to discriminative models, where classifiers are exploited to distinguish mCs from other backgrounds. However, it is still challenging for these methods to tell the mCs from amounts of normal tissues because they are too tiny (at most 14 pixels). Generative methods can precisely model the normal tissues and regard the abnormal ones as outliers, while they fail to further distinguish the mCs from other anomalies, ie vessel calcifications. In this paper, we propose a hybrid approach by taking advantages of both generative and discriminative models. Firstly, a generative model named Anomaly Separation Network (ASN) is used to generate candidate mCs. ASN contains two major components. A deep convolutional encoder-decoder network is built to learn the image reconstruction mapping and a t-test loss function is designed to separate the distributions of the reconstruction residuals of mCs from normal tissues. Secondly, a discriminative model is cascaded to tell the mCs from the false positives. Finally, to verify the effectiveness of our method, we conduct experiments on both public and in-house datasets, which demonstrates that our approach outperforms previous state-of-the-art methods.
</details>

## 3. Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù<div id = "s3"></div>

-   \*[\[Tpami 2024\]](https://ieeexplore.ieee.org/abstract/document/10553645/)**Moodv2: Ù†Ù…Ø°Ø¬Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ØªÙˆØ²ÙŠØ¹**[: Octocat:](https://github.com/dvlab-research/MOOD)

    _L I ØŒ Jin Ùˆ Chen ØŒ Peng Ùˆ Andy U ØŒ SÙ‡ÙŠÙ„ Ùˆ L IU ØŒ Shuan DJ IA ØŒ J ia_

<div align="center">
  <img src="https://github.com/user-attachments/assets/92cb8811-28e9-4b49-b7d2-38789966f9ba" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
The crux of effective out-of-distribution (OOD) detection lies in acquiring a robust in-distribution (ID) representation, distinct from OOD samples. While previous methods predominantly leaned on recognition-based techniques for this purpose, they often resulted in shortcut learning, lacking comprehensive representations. In our study, we conducted a comprehensive analysis, exploring distinct pretraining tasks and employing various OOD score functions. The results highlight that the feature representations pre-trained through reconstruction yield a notable enhancement and narrow the performance gap among various score functions. This suggests that even simple score functions can rival complex ones when leveraging reconstruction-based pretext tasks. Reconstruction-based pretext tasks adapt well to various score functions. As such, it holds promising potential for further expansion. Our OOD detection framework, MOODv2, employs the masked image modeling pretext task. Without bells and whistles, MOODv2 impressively enhances 14.30% AUROC to 95.68% on ImageNet and achieves 99.98% on CIFAR-10.
</details>

-   \*[\[CV PR 2023\]](http://openaccess.thecvf.com/content/CVPR2023/html/Li_Rethinking_Out-of-Distribution_OOD_Detection_Masked_Image_Modeling_Is_All_You_CVPR_2023_paper.html)**Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ØªÙˆØ²ÙŠØ¹ (OOD): Ù†Ù…Ø°Ø¬Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ù‡ÙŠ ÙƒÙ„ Ù…Ø§ ØªØ­ØªØ§Ø¬Ù‡**[: Octocat:](https://github.com/dvlab-research/MOOD)

    _L I ØŒ Jin and Chen ØŒ Peng Guang''s He ØŒ Z Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ø§Ø´Ù…Ø¦Ø²Ø§Ø² Ø£Ù†Ø¯ÙŠ ÙŠÙˆ ØŒ Ø³Ù‡Ù„Ø©_

<div align="center">
  <img src="https://github.com/user-attachments/assets/8a299493-328b-46a0-96ee-93140ce9586e" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Medical anomaly detection aims to identify abnormal findings using only normal training data, playing a crucial role in health screening and recognizing rare diseases. Reconstruction-based methods, particularly those utilizing autoencoders (AEs), are dominant in this field. They work under the assumption that AEs trained on only normal data cannot reconstruct unseen abnormal regions well, thereby enabling the anomaly detection based on reconstruction errors. However, this assumption does not always hold due to the mismatch between the reconstruction training objective and the anomaly detection task objective, rendering these methods theoretically unsound. This study focuses on providing a theoretical foundation for AE-based reconstruction methods in anomaly detection. By leveraging information theory, we elucidate the principles of these methods and reveal that the key to improving AE in anomaly detection lies in minimizing the information entropy of latent vectors. Experiments on four datasets with two image modalities validate the effectiveness of our theory. To the best of our knowledge, this is the first effort to theoretically clarify the principles and design philosophy of AE for anomaly detection. The code is available at https://github.com/caiyu6666/AE4AD.
</details>

-   [\[ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù… 2023\]](https://arxiv.org/pdf/2301.08330)**Ø¯ÙˆØ± Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ° ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ©**[: Octocat:](https://github.com/antanaskascenas/denoisingae)

    _Kascenas Ùˆ Antanas Ùˆ Sanchez Ùˆ Pedro Ùˆ Schrempf Ùˆ Patrick Ùˆ Wang Ùˆ Chaoyang Ùˆ Clackett Ùˆ William Ùˆ Mikhael Ùˆ Shadia S Ùˆ Voisey Ùˆ Jeremy P Ùˆ Goatman Ùˆ Keith Ùˆ Weir Ùˆ Alexander Ùˆ Pugeault Ùˆ Nicolas ÙˆØºÙŠØ±Ù‡Ø§_

<div align="center">
  <img src="https://github.com/user-attachments/assets/eb0d29f9-1697-423a-83c3-a111a3097182" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Pathological brain lesions exhibit diverse appearance in brain images, in terms of intensity, texture, shape, size, and location. Comprehensive sets of data and annotations are difficult to acquire. Therefore, unsupervised anomaly detection approaches have been proposed using only normal data for training, with the aim of detecting outlier anomalous voxels at test time. Denoising methods, for instance classical denoising autoencoders (DAEs) and more recently emerging diffusion models, are a promising approach, however naive application of pixelwise noise leads to poor anomaly detection performance. We show that optimization of the spatial resolution and magnitude of the noise improves the performance of different model training regimes, with similar noise parameter adjustments giving good performance for both DAEs and diffusion models. Visual inspection of the reconstructions suggests that the training noise influences the trade-off between the extent of the detail that is reconstructed and the extent of erasure of anomalies, both of which contribute to better anomaly detection performance. We validate our findings on two real-world datasets (tumor detection in brain MRI and hemorrhage/ischemia/tumor detection in brain CT), showing good detection on diverse anomaly appearances. Overall, we find that a DAE trained with coarse noise is a fast and simple method that gives state-of-the-art accuracy. Diffusion models applied to anomaly detection are as yet in their infancy and provide a promising avenue for further research. Code for our DAE model and coarse noise is provided at: https://github.com/AntanasKascenas/DenoisingAE.
</details>
  
## 4. AE-Based Approaches <div id = "s4"></div>

-   [\[MIC CAI 2024\]](https://arxiv.org/pdf/2403.09303)**Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ø·Ø¨ÙŠ Ù…Ù† Ù…Ù†Ø¸ÙˆØ± Ù†Ø¸Ø±ÙŠ**[: Octocat:](https://github.com/caiyu6666/ae4ad)

    _Cai ØŒ Chen ØŒ H Ao Ùˆ Cheng of Yuan ØŒ K-Tning_

<div align="center">
  <img src="https://github.com/user-attachments/assets/25d5c2b4-d2d9-4d1a-a90e-c8651dc01f9d" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Medical anomaly detection aims to identify abnormal findings using only normal training data, playing a crucial role in health screening and recognizing rare diseases. Reconstruction-based methods, particularly those utilizing autoencoders (AEs), are dominant in this field. They work under the assumption that AEs trained on only normal data cannot reconstruct unseen abnormal regions well, thereby enabling the anomaly detection based on reconstruction errors. However, this assumption does not always hold due to the mismatch between the reconstruction training objective and the anomaly detection task objective, rendering these methods theoretically unsound. This study focuses on providing a theoretical foundation for AE-based reconstruction methods in anomaly detection. By leveraging information theory, we elucidate the principles of these methods and reveal that the key to improving AE in anomaly detection lies in minimizing the information entropy of latent vectors. Experiments on four datasets with two image modalities validate the effectiveness of our theory. To the best of our knowledge, this is the first effort to theoretically clarify the principles and design philosophy of AE for anomaly detection. The code is available at https://github.com/caiyu6666/AE4AD.
</details>
  
- [[ICLR 2023]](https://openreview.net/pdf?id=9OmCr1q54Z) **AE-FLOW: Autoencoders with Normalizing Flows for Medical Images Anomaly Detection**

_Zhao ØŒ y u- Ùˆ ding ØŒ q ia o-chocking and Zhang ØŒ Î¾ AO Group_

<div align="center">
  <img src="https://github.com/user-attachments/assets/cb07b128-80eb-4c1b-b606-664e2a866675" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Anomaly detection from medical images is an important task for clinical screening and diagnosis. In general, a large dataset of normal images are available while only few abnormal images can be collected in clinical practice. By mimicking the diagnosis process of radiologists, we attempt to tackle this problem by learning a tractable distribution of normal images and identify anomalies by differentiating the original image and the reconstructed normal image. More specifically, we propose a normalizing flow-based autoencoder for an efficient and tractable representation of normal medical images. The anomaly score consists of the likelihood originated from the normalizing flow and the reconstruction error of the autoencoder, which allows to identify the abnormality and provide an interpretability at both image and pixel levels. Experimental evaluation on two medical images datasets showed that the proposed model outperformed the other approaches by a large margin, which validated the effectiveness and robustness of the proposed method.
</details>
  
## 5. GAN-Based Approaches <div id = "s5"></div>

-   [\[Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø¹ØµØ¨ÙŠØ© 2025\]](https://www.sciencedirect.com/science/article/pii/S0925231224015339)**Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ø·Ø¨ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø®ØµÙˆÙ…Ø© Ø§Ù„Ù…ØªØ³Ù‚Ø© Ù„Ù„Ø¯ÙˆØ±Ø©**[: Octocat:](https://github.com/valdelch/cyclegans-anomalydetection)

    _Ø¨ÙˆØ¬Ù‡Ø§Ù… ØŒ ÙØ§Ù„Ù†ØªÙŠÙ† ØŒ ÙØ§Ù„Ù†ØªÙŠÙ† ØŒ Ø¨ÙŠÙ†ÙˆØ§ ØŒ Ø¨ÙŠÙ†ÙˆØ§._

<div align="center">
  <img src="https://github.com/user-attachments/assets/f05671aa-c0d5-45a8-a4a7-9afec22e84a5" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
In this study, a new Anomaly Detection (AD) approach for industrial and medical images is proposed. This method leverages the theoretical strengths of unsupervised learning and the data availability of both normal and abnormal classes. Indeed, the AD is often formulated as an unsupervised task, implying only normal images during training. These normal images are devoted to be reconstructed through an autoencoder architecture, for instance. However, the information contained in abnormal data, when available, is also valuable for this reconstruction. The model would be able to identify its weaknesses by also learning how to transform an abnormal image into a normal one. This abnormal-to-normal reconstruction helps the entire model to learn better than a single normal-to-normal reconstruction. To be able to exploit abnormal images, the proposed method uses Cycle-Generative Adversarial Networks (Cycle-GAN) for (ab)normal-to-normal translation. After an input image has been reconstructed by the normal generator, an anomaly score quantifies the differences between the input and its reconstruction. Based on a threshold set to satisfy a business quality constraint, the input image is then flagged as normal or not. The proposed method is evaluated on industrial and medical datasets. The results demonstrate accurate performance with a zero false negative constraint compared to state-of-the-art methods. Quantitatively, our method reaches an accuracy under a zero false negative constraint of 79.89%, representing an improvement of about 17% compared to competitors. The code is available at https://github.com/ValDelch/CycleGANS-AnomalyDetection.
</details>
  
- [[PR 2024]](https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/3178214/2229397_Article_.pdf?sequence=1) **Anomaly Detection via Gating Highway Connection for Retinal Fundus Images** [:octocat:](https://github.com/WentianZhang-ML/GatingAno)

_Zhang ØŒ Wen Tianan DL IU ØŒ Ha O Ùˆ X IE ØŒ Jin Heng Ùˆ Huang ØŒ Ya and Zhang ØŒ Yuan Dli ØŒ Y Ue Xiangan Chandra ØŒ RA Ø¥Ù„Ù‰ Haven's RA and Zheng ØŒ Y E Peak_

<div align="center">
  <img src="https://github.com/user-attachments/assets/b017215e-0998-4e63-97f9-7a5271756c68" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Since the labels for medical images are challenging to collect in real scenarios, especially for rare diseases, fully supervised methods cannot achieve robust performance for clinical anomaly detection. Recent research tried to tackle this problem by training the anomaly detection framework using only normal data. Reconstruction-based methods, e.g., auto-encoder, achieved impressive performances in the anomaly detection task. However, most existing methods adopted the straightforward backbone architecture (i.e., encoder-and-decoder) for image reconstruction. The design of a skip connection, which can directly transfer information between the encoder and decoder, is rarely used. Since the existing U-Net has demonstrated the effectiveness of skip connections for image reconstruction tasks, in this paper, we first use the dynamic gating strategy to achieve the usage of skip connections in existing reconstruction-based anomaly detection methods and then propose a novel gating highway connection module to adaptively integrate skip connections into the framework and boost its anomaly detection performance, namely GatingAno. Furthermore, we formulate an auxiliary task, namely histograms of oriented gradients (HOG) prediction, to encourage the framework to exploit contextual information from fundus images in a self-driven manner, which increases the robustness of feature representation extracted from the healthy samples. Last but not least, to improve the model generalization for anomalous data, we introduce an adversarial strategy for the training of our multi-task framework. Experimental results on the publicly available datasets, i.e., IDRiD and ADAM, validate the superiority of our method for detecting abnormalities in retinal fundus images. The source code is available at https://github.com/WentianZhang-ML/GatingAno.
</details>
  
- [[CVPR 2023]](http://openaccess.thecvf.com/content/CVPR2023/papers/Xiang_SQUID_Deep_Feature_In-Painting_for_Unsupervised_Anomaly_Detection_CVPR_2023_paper.pdf) **SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection** [:octocat:](https://github.com/tiangexiang/squid)

_Î¾ang ØŒ tia n Ùˆ Zhang ØŒ yi xiaoan dl u ØŒ yong with Andy UI ØŒ Alan Land Zhang ØŒ C Good Next and Cai ØŒ Weidong and Zhou ØŒ Z Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù…ÙŠØ¹_

<div align="center">
  <img src="https://github.com/user-attachments/assets/8ed73cd7-6b22-4e7e-be06-4d5c97e7ab80" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Radiography imaging protocols focus on particular body regions, therefore producing images of great similarity and yielding recurrent anatomical structures across patients. To exploit this structured information, we propose the use of Space-aware Memory Queues for In-painting and Detecting anomalies from radiography images (abbreviated as SQUID). We show that SQUID can taxonomize the ingrained anatomical structures into recurrent patterns; and in the inference, it can identify anomalies (unseen/modified patterns) in the image. SQUID surpasses 13 state-of-the-art methods in unsupervised anomaly detection by at least 5 points on two chest X-ray benchmark datasets measured by the Area Under the Curve (AUC). Additionally, we have created a new dataset (DigitAnatomy), which synthesizes the spatial correlation and consistent shape in chest anatomy. We hope DigitAnatomy can prompt the development, evaluation, and interpretability of anomaly detection methods.
</details>

-   [\[Media 2019\]](https://www.sciencedirect.com/science/article/pii/S1361841518302640)**F-Anogan: Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ø³Ø±ÙŠØ¹ ØºÙŠØ± Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù Ù…Ø¹ Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø®ØµÙˆÙ…Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠØ©**[: Octocat:](https://github.com/A03ki/f-AnoGAN)

    _Ø§Ù„Ø±Ø®ÙˆÙŠØ§Øª ØŒ ØªÙˆÙ…Ø§Ø³ ÙˆØ³ÙŠÙ„ÙÙŠÙ„ ØŒ ÙÙŠÙ„ÙŠØ¨ ÙˆÙˆØ§Ù„Ø¯Ø³ØªÙŠÙ† ØŒ Ø³ÙŠØ¨Ø§Ø³ØªÙŠØ§Ù† Ù… ÙˆÙ„ÙˆÙ†Ø¬Ø² ØŒ Ø¬ÙˆØ±Ø¬ Ùˆ Smurten ØŒ Ø£ÙˆØ±Ø³ÙˆÙ„Ø§ ØŒ Ø£ÙˆØ±Ø³ÙˆÙ„Ø§ ØŒ Ø£ÙˆØ±Ø³ÙˆÙ„Ø§ ØŒ Ø£ÙˆØ±Ø³ÙˆÙ„Ø§_

<div align="center">
  <img src="https://github.com/user-attachments/assets/1a78902d-3748-4018-b076-2373353f0034" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Obtaining expert labels in clinical imaging is difficult since exhaustive annotation is time-consuming. Furthermore, not all possibly relevant markers may be known and sufficiently well described a priori to even guide annotation. While supervised learning yields good results if expert labeled training data is available, the visual variability, and thus the vocabulary of findings, we can detect and exploit, is limited to the annotated lesions. Here, we present fast AnoGAN (f-AnoGAN), a generative adversarial network (GAN) based unsupervised learning approach capable of identifying anomalous images and image segments, that can serve as imaging biomarker candidates. We build a generative model of healthy training data, and propose and evaluate a fast mapping technique of new data to the GANâ€™s latent space. The mapping is based on a trained encoder, and anomalies are detected via a combined anomaly score based on the building blocks of the trained model â€“ comprising a discriminator feature residual error and an image reconstruction error. In the experiments on optical coherence tomography data, we compare the proposed method with alternative approaches, and provide comprehensive empirical evidence that f-AnoGAN outperforms alternative approaches and yields high anomaly detection accuracy. In addition, a visual Turing test with two retina experts showed that the generated images are indistinguishable from real normal retinal OCT images. The f-AnoGAN code is available at https://github.com/tSchlegl/f-AnoGAN.
</details>

-   [\[IPMI 2017\]](https://link.springer.com/chapter/10.1007/978-3-319-59050-9_12)**Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø°ÙˆØ° ØºÙŠØ± Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù Ù…Ø¹ Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø®ØµÙˆÙ…Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠØ© Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø¹Ù„Ø§Ù…Ø©**[: Octocat:](https://github.com/seungjunlee96/AnoGAN-pytorch)

_Slippel Ùˆ Thomas Ùˆ Sillek Ùˆ Philipp Ùˆ Waldstin Ùˆ Sebastian M Ùˆ Small-Enliderth Ùˆ Ursula and Lakes Ùˆ Georg_

<div align="center">
  <img src="https://github.com/user-attachments/assets/690968ff-f446-400e-8348-68b2cc913680" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Obtaining models that capture imaging markers relevant for disease progression and treatment monitoring is challenging. Models are typically based on large amounts of data with annotated examples of known markers aiming at automating detection. High annotation effort and the limitation to a vocabulary of known markers limit the power of such approaches. Here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers. We propose AnoGAN, a deep convolutional generative adversarial network to learn a manifold of normal anatomical variability, accompanying a novel anomaly scoring scheme based on the mapping from image space to a latent space. Applied to new data, the model labels anomalies, and scores image patches indicating their fit into the learned distribution. Results on optical coherence tomography images of the retina demonstrate that the approach correctly identifies anomalous images, such as images containing retinal fluid or hyperreflective foci.
</details>

## 6. Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¯ÙÙ‚<div id = "s6"></div>

-   \*[\[CV PR 2023\]](https://openaccess.thecvf.com/content/CVPR2023/html/Lei_PyramidFlow_High-Resolution_Defect_Contrastive_Localization_Using_Pyramid_Normalizing_Flow_CVPR_2023_paper.html?ref=https://githubhelp.com)**Ø§Ù„Ù‡Ø±Ù…: ØªÙˆØ·ÙŠÙ† Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¯Ù‚Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ¯ÙÙ‚ ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù‡Ø±Ù…**[: Octocat:](https://github.com/gasharper/PyramidFlow)

    _Lei ØŒ J ia Rui Ùˆ Hu ØŒ Î¾obo Ùˆ Wang ØŒ Y ue Ùˆ l iu ØŒ dong_

<div align="center">
  <img src="https://github.com/user-attachments/assets/758827c6-a845-4e1c-94b6-bbe7e3ebcdbf" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
During industrial processing, unforeseen defects may arise in products due to uncontrollable factors. Although unsupervised methods have been successful in defect localization, the usual use of pre-trained models results in low-resolution outputs, which damages visual performance. To address this issue, we propose PyramidFlow, the first fully normalizing flow method without pre-trained models that enables high-resolution defect localization. Specifically, we propose a latent template-based defect contrastive localization paradigm to reduce intra-class variance, as the pre-trained models do. In addition, PyramidFlow utilizes pyramid-like normalizing flows for multi-scale fusing and volume normalization to help generalization. Our comprehensive studies on MVTecAD demonstrate the proposed method outperforms the comparable algorithms that do not use external priors, even achieving state-of-the-art performance in more challenging BTAD scenarios.
</details>

## 7. Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø±<div id = "s7"></div>

-   [\[CV PR 2025\]](https://arxiv.org/abs/2406.01078)**Ø´Ø°ÙˆØ° Ø£ÙŠ Ø´ÙŠØ¡: Ø¬ÙŠÙ„ Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ø¨ØµØ±ÙŠ ØºÙŠØ± Ø§Ù„Ù…Ø±Ø¦ÙŠ**[: Octocat:](https://github.com/EPFL-IMOS/AnomalyAny)

    _ØµÙ† ØŒ Ù‡Ø§Ù† ÙˆÙƒØ§Ùˆ ØŒ ÙŠÙˆÙ†ØºØ§Ù†Øº ÙˆØ¯ÙˆÙ†Øº ØŒ Ù‡Ø§Ùˆ ÙˆÙÙ†Ùƒ ØŒ Ø£ÙˆÙ„ØºØ§_

<div align="center">
  <img src="https://github.com/user-attachments/assets/dfc56b79-f820-49dd-9d38-683836852289" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Visual anomaly detection (AD) presents significant challenges due to the scarcity of anomalous data samples. While numerous works have been proposed to synthesize anomalous samples, these synthetic anomalies often lack authenticity or require extensive training data, limiting their applicability in real-world scenarios. In this work, we propose Anomaly Anything (AnomalyAny), a novel framework that leverages Stable Diffusion (SD)â€™s image generation capabilities to generate diverse and realistic unseen anomalies. By conditioning on a single normal sample during test time, AnomalyAny is able to generate unseen anomalies for arbitrary object types with text descriptions. Within AnomalyAny, we propose attention-guided anomaly optimization to direct SDâ€™s attention on generating hard anomaly concepts. Additionally, we introduce prompt-guided anomaly refinement, incorporating detailed descriptions to further improve the generation quality. Extensive experiments on MVTec AD and VisA datasets demonstrate AnomalyAnyâ€™s ability in generating high-quality unseen anomalies and its effectiveness in enhancing downstream AD performance. Our demo and code are available at https://hansunhayden.github.io/AnomalyAny.github.io/.
</details>

-   [\[WACV 2025\]](https://www.researchgate.net/profile/Sudipta-Roy-9/publication/389540571_Self-Supervised_Anomaly_Segmentation_via_Diffusion_Models_with_Dynamic_Transformer_UNet/links/67c6e067461fb56424f04c9f/Self-Supervised-Anomaly-Segmentation-via-Diffusion-Models-with-Dynamic-Transformer-UNet.pdf)**ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„ØªÙŠ ÙŠØªÙ… Ø¥Ø´Ø±Ø§ÙÙ‡Ø§ Ø°Ø§ØªÙŠØ§ Ø¹Ø¨Ø± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ù…Ø¹ Ù…Ø­ÙˆÙ„ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ unet**[: Octocat:](https://github.com/MAXNORM8650/Annotsim)

    _ÙƒÙˆÙ…Ø§Ø± ØŒ ÙƒÙˆÙ…Ø§Ù„ ÙˆØ´Ø§ÙƒØ±Ø§Ø¨ÙˆØ±ØªÙŠ ØŒ Ø³Ù†ÙŠØ´ÙŠØ³ ÙˆÙ…Ø§Ù‡Ø§Ø¨Ø§ØªØ±Ø§ ØŒ Ø¯ÙˆØ§Ø±ÙƒØ§Ù†Ø§Ø« ÙˆØ¨ÙˆØ¬ÙˆØ±ØºØ§Ø± ØŒ Ø¨ÙŠØ²Ø§Ø¯ ÙˆØ±ÙˆÙŠ ØŒ Ø³ÙˆØ¯ÙŠØ¨ØªØ§_

<div align="center">
  <img src="https://github.com/user-attachments/assets/9e601d52-d982-437e-9b69-b1ade659d54b" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
A robust anomaly detection mechanism should possess the capability to effectively remediate anomalies, restoring them to a healthy state, while preserving essential healthy information. Despite the efficacy of existing generative models in learning the underlying distribution of healthy reference data, they face primary challenges when it comes to efficiently repair larger anomalies or anomalies situated near high pixel-density regions. In this paper, we introduce a self-supervised anomaly detection method based on a diffusion model that samples from multi-frequency, four-dimensional simplex noise and makes predictions using our proposed Dynamic Transformer UNet (DTUNet). This simplex-based noise function helps address primary problems to some extent and is scalable for three-dimensional and colored images. In the evolution of ViT, our developed architecture serving as the backbone for the diffusion model, is tailored to treat time and noise image patches as tokens. We incorporate long skip connections bridging the shallow and deep layers, along with smaller skip connections within these layers. Furthermore, we integrate a partial diffusion Markov process, which reduces sampling time, thus enhancing scalability. Our method surpasses existing generative-based anomaly detection methods across three diverse datasets, which include BrainMRI, Brats2021, and the MVtec dataset. It achieves an average improvement of +10.1% in Dice coefficient, +10.4% in IOU, and +9.6% in AUC. Our source code is made publicly available on Github.
</details>
  
- [[TMI 2024]](https://arxiv.org/pdf/2308.02062) **Diffusion Models for Counterfactual Generation and Anomaly Detection in Brain lmages** [:octocat:](https://github.com/alessandro-f/dif-fuse)

_Fontanella ØŒ Alessandro Ùˆ Mair ØŒ Grant and Wardlaw ØŒ Joanna and Trucco ØŒ Emanuele and Storkey ØŒ Amos_

<div align="center">
  <img src="https://github.com/user-attachments/assets/48f34db0-8ffb-494d-830f-8ce4b79bd00e" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Segmentation masks of pathological areas are useful in many medical applications, such as brain tumour and stroke management. Moreover, healthy counterfactuals of diseased images can be used to enhance radiologistsâ€™ training files and to improve the interpretability of segmentation models. In this work, we present a weakly supervised method to generate a healthy version of a diseased image and then use it to obtain a pixel-wise anomaly map. To do so, we start by considering a saliency map that approximately covers the pathological areas, obtained with ACAT. Then, we propose a technique that allows to perform targeted modifications to these regions, while preserving the rest of the image. In particular, we employ a diffusion model trained on healthy samples and combine Denoising Diffusion Probabilistic Model (DDPM) and Denoising Diffusion Implicit Model (DDIM) at each step of the sampling process. DDPM is used to modify the areas affected by a lesion within the saliency map, while DDIM guarantees reconstruction of the normal anatomy outside of it. The two parts are also fused at each timestep, to guarantee the generation of a sample with a coherent appearance and a seamless transition between edited and unedited parts. We verify that when our method is applied to healthy samples, the input images are reconstructed without significant modifications. We compare our approach with alternative weakly supervised methods on the task of brain lesion segmentation, achieving the highest mean Dice and IoU scores among the models considered.
</details>
  
- [[MICCAI 2024]](https://arxiv.org/pdf/2403.08464) **Diffusion Models with Implicit Guidance for Medical Anomaly Detection** [:octocat:](https://github.com/ci-ber/thor_ddpm)

_Bercea ØŒ Cosmin I Ùˆ Wiestler ØŒ Benedikt Ùˆ Rueckert ØŒ Ø¯Ø§Ù†ÙŠØ§Ù„ ÙˆØ´Ù†Ø§Ø¨Ù„ ØŒ Ø¬ÙˆÙ„ÙŠØ§ Ø£_

<div align="center">
  <img src="https://github.com/user-attachments/assets/b5860d4f-eaaf-44bf-a0ea-1279f7d35446" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Diffusion models have advanced unsupervised anomaly detection by improving the transformation of pathological images into pseudo-healthy equivalents. Nonetheless, standard approaches may compromise critical information during pathology removal, leading to restorations that do not align with unaffected regions in the original scans. Such discrepancies can inadvertently increase false positive rates and reduce specificity, complicating radiological evaluations. This paper introduces Temporal Harmonization for Optimal Restoration (THOR), which refines the reverse diffusion process by integrating implicit guidance through intermediate masks. THOR aims to preserve the integrity of healthy tissue details in reconstructed images, ensuring fidelity to the original scan in areas unaffected by pathology. Comparative evaluations reveal that THOR surpasses existing diffusion-based methods in retaining detail and precision in image restoration and detecting and segmenting anomalies in brain MRIs and wrist X-rays. Code: https://github.com/compai-lab/2024-miccai-bercea-thor.git.
</details>

-   [\[ØºÙŠØ± Ù…ØªØ£ÙƒØ¯ 2024\]](https://link.springer.com/chapter/10.1007/978-3-031-73158-7_11)**Ù†Ù…Ø§Ø°Ø¬ Ù†Ø´Ø± Ù…ÙƒÙŠÙØ© Ø§Ù„ØµÙˆØ± Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ø·Ø¨ÙŠ**

    _Ø¨ÙˆØº ØŒ Ù…Ø§Ø«ÙŠÙˆ ÙˆØ±ÙŠÙ†Ø§ÙˆØ¯ ØŒ Ù‡Ø§Ø¯Ø±ÙŠÙ† ÙˆÙ…Ø§Ø±ÙŠÙ…ÙˆÙ†Øª ØŒ Ø³ÙŠØ±Ø¬ÙŠÙˆ Ù†Ø§ÙÙŠØ§Ù„ ÙˆØ´ÙŠÙƒÙ†ÙŠÙƒØ§ ØŒ Ø³Ø§Ø±Ø© Ùˆ M {\\ "U} Ller ØŒ Johanna P and Tarroni ØŒ Giacomo and Kainz ØŒ Bernhard_

<div align="center">
  <img src="https://github.com/user-attachments/assets/e4bd523f-f71e-45ed-825c-9024144a6fdc" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Generating pseudo-healthy reconstructions of images is an effective way to detect anomalies, as identifying the differences between the reconstruction and the original can localise arbitrary anomalies whilst also providing interpretability for an observer by displaying what the image â€˜shouldâ€™ look like. All existing reconstruction-based methods have a common shortcoming; they assume that models trained on purely normal data are incapable of reproducing pathologies yet also able to fully maintain healthy tissue. These implicit assumptions often fail, with models either not recovering normal regions or reproducing both the normal and abnormal features. We rectify this issue using image-conditioned diffusion models. Our model takes the input image as conditioning and is explicitly trained to correct synthetic anomalies introduced into healthy images, ensuring that it removes anomalies at test time. This conditioning allows the model to attend to the entire image without any loss of information, enabling it to replicate healthy regions with high fidelity. We evaluate our method across four datasets and define a new state-of-the-art performance for residual-based anomaly detection. Code is available at https://github.com/matt-baugh/img-cond-diffusion-model-ad.
</details>

-   [\[CV PR 2022\]](https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Wyatt_AnoDDPM_Anomaly_Detection_With_Denoising_Diffusion_Probabilistic_Models_Using_Simplex_CVPRW_2022_paper.pdf)**Ø£Ù†ÙˆØ¯ ÙÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©: Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø°ÙˆØ° Ù…Ø¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù„Ù†Ø´Ø± Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¶ÙˆØ¶Ø§Ø¡ Ø¨Ø³ÙŠØ·Ø©**[: Octocat:](https://github.com/julian-wyatt/anoddpm)

    _ÙˆØ§ÙŠØª ØŒ Ø¬ÙˆÙ„ÙŠØ§Ù† ÙˆÙ„ÙŠØªØ´ â€‹â€‹ØŒ Ø¢Ø¯Ù… ÙˆØ´Ù…ÙˆÙ† ØŒ Ø³ÙŠØ¨Ø§Ø³ØªÙŠØ§Ù† Ù…. ÙˆÙŠÙ„ÙƒÙˆÙƒØ³ ØŒ ÙƒØ±ÙŠØ³ Ø¬ÙŠ_

<div align="center">
  <img src="https://github.com/user-attachments/assets/5e738dcc-ba84-4446-b996-9cc493f37bb9" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Generative models have been shown to provide a powerful mechanism for anomaly detection by learning to model healthy or normal reference data which can subsequently be used as a baseline for scoring anomalies. In this work we consider denoising diffusion probabilistic models (DDPMs) for unsupervised anomaly detection. DDPMs have superior mode coverage over generative adversarial networks (GANs) and higher sample quality than variational autoencoders (VAEs). However, this comes at the expense of poor scalability and increased sampling times due to the long Markov chain sequences required. We observe that within reconstruction-based anomaly detection a full-length Markov chain diffusion is not required. This leads us to develop a novel partial diffusion anomaly detection strategy that scales to high-resolution imagery, named AnoDDPM. A secondary problem is that Gaussian diffusion fails to capture larger anomalies; therefore we develop a multi-scale simplex noise diffusion process that gives control over the target anomaly size. AnoDDPM with simplex noise is shown to significantly outperform both f-AnoGAN and Gaussian diffusion for the tumorous dataset of 22 T1-weighted MRI scans (CCBS Edinburgh) qualitatively and quantitatively (improvement of+ 25.5% Sorensen-Dice coefficient,+ 17.6% IoU and+ 7.4% AUC).
</details>
  
- [[MICCAI 2022]](https://arxiv.org/pdf/2203.04306) **Diffusion Models for Medical Anomaly Detection** [:octocat:](https://github.com/JuliaWolleb/diffusion-anomaly)

_Wolleb ØŒ Ø¬ÙˆÙ„ÙŠØ§ ÙˆØ¨ÙŠØ¯Ø± ØŒ ÙÙ„ÙˆØ±Ù†ØªÙŠÙ† ÙˆØ³Ø§Ù†Ø¯ÙƒÙˆÙ„Ø± ØŒ Ø±ÙˆØ¨Ù† Ùˆ Cattin ØŒ ÙÙŠÙ„ÙŠØ¨ Ø³ÙŠ_

<div align="center">
  <img src="https://github.com/user-attachments/assets/d26c0351-cf7d-4e8c-84d4-66f081f370ad" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
In medical applications, weakly supervised anomaly detection methods are of great interest, as only image-level annotations are required for training. Current anomaly detection methods mainly rely on generative adversarial networks or autoencoder models. Those models are often complicated to train or have difficulties to preserve fine details in the image. We present a novel weakly supervised anomaly detection method based on denoising diffusion implicit models. We combine the deterministic iterative noising and denoising scheme with classifier guidance for image-to-image translation between diseased and healthy subjects. Our method generates very detailed anomaly maps without the need for a complex training procedure. We evaluate our method on the BRATS2020 dataset for brain tumor detection and the CheXpert dataset for detecting pleural effusions.
</details>

## 8. Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØµØ­ÙŠØ­<div id = "s8"></div>

-   [\[MIDL 2024\]](https://proceedings.mlr.press/v227/behrendt24a/behrendt24a.pdf)**Ù†Ù…Ø§Ø°Ø¬ Ø§Ù†ØªØ´Ø§Ø± Ù…ØµØ­Ø­Ø© Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ° ØºÙŠØ± Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø±Ù‚Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ø¯Ù…Ø§Øº Ø§Ù„ØªØµÙˆÙŠØ± Ø¨Ø§Ù„Ø±Ù†ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ**[: Octocat:](https://github.com/finnbehrendt/patched-diffusion-models-uad)

    _Behrendt ØŒ Finn and Bhattcharya ØŒ Debayan and KrÃ¼ger ØŒ Julia and Octim ØŒ Roland and Schlaefer ØŒ Alexander_

<div align="center">
  <img src="https://github.com/user-attachments/assets/e5a30138-71c4-4a12-a78b-5ae3c48a3c59" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
The use of supervised deep learning techniques to detect pathologies in brain MRI scans can be challenging due to the diversity of brain anatomy and the need for annotated data sets. An alternative approach is to use unsupervised anomaly detection, which only requires sample-level labels of healthy brains to create a reference representation. This reference representation can then be compared to unhealthy brain anatomy in a pixel-wise manner to identify abnormalities. To accomplish this, generative models are needed to create anatomically consistent MRI scans of healthy brains. While recent diffusion models have shown promise in this task, accurately generating the complex structure of the human brain remains a challenge. In this paper, we propose a method that reformulates the generation task of diffusion models as a patch-based estimation of healthy brain anatomy, using spatial context to guide and improve reconstruction. We evaluate our approach on data of tumors and multiple sclerosis lesions and demonstrate a relative improvement of 25.1% compared to existing baselines.
</details>

-   \*[\[CV PR 2021\]](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Glancing_at_the_Patch_Anomaly_Localization_With_Global_and_Local_CVPR_2021_paper.html?ref=https://githubhelp.com)**Ø¥Ù„Ù‚Ø§Ø¡ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØµØ­ÙŠØ­: ØªÙˆØ·ÙŠÙ† Ø§Ù„Ø´Ø°ÙˆØ° Ù…Ø¹ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ÙˆØ§Ù„Ù…Ø­Ù„ÙŠØ©**

    _Wang ØŒ S ÙŠØ³ØªØ­Ù‚ Ù„Ù„ØºØ§ÙŠØ© ÙˆÙ„Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø£Ù…Ø± ÙƒØ°Ù„Ùƒ_

<div align="center">
  <img src="https://github.com/user-attachments/assets/eeb0aba8-750e-41c5-8b91-31241f274a71" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Anomaly localization, with the purpose to segment the anomalous regions within images, is challenging due to the large variety of anomaly types. Existing methods typically train deep models by treating the entire image as a whole yet put little effort into learning the local distribution, which is vital for this pixel-precise task. In this work, we propose an unsupervised patch-based approach that gives due consideration to both the global and local information. More concretely, we employ a Local-Net and Global-Net to extract features from any individual patch and its surrounding respectively. Global-Net is trained with the purpose to mimic the local feature such that we can easily detect an abnormal patch when its feature mismatches that from the context. We further introduce an Inconsistency Anomaly Detection (IAD) head and a Distortion Anomaly Detection (DAD) head to sufficiently spot the discrepancy between global and local features. A scoring function derived from the multi-head design facilitates high-precision anomaly localization. Extensive experiments on a couple of real-world datasets suggest that our approach outperforms state-of-the-art competitors by a sufficiently large margin.
</details>

## 9. Ø§Ù„Ø§Ù†ØµÙ‡Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·<div id = "s9"></div>

-   [\[Inf. Ø§Ù„Ø§Ù†ØµÙ‡Ø§Ø± 2025\]](https://www.sciencedirect.com/science/article/pii/S1566253524004093)**ØªÙƒÙŠÙŠÙ Ù†Ù…ÙˆØ°Ø¬ Ø£ÙŠ Ø´ÙŠØ¡ Ù„Ø§ÙƒØªØ´Ø§Ù ÙˆØªÙˆØ·ÙŠÙ† Ø´Ø°ÙˆØ° Ø§Ù„Ø´Ø¨ÙƒÙŠØ© Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·**[: Octocat:](https://github.com/Jingtao-Li-CVer/MMRAD)

    _L I ØŒ Jin G Set and Chen ØŒ Ting and Wang ØŒ X In and Z Hong ØŒ Y Press Non-Press D Î¾o ØŒ X U Press_

<div align="center">
  <img src="https://github.com/user-attachments/assets/73accda9-cf6e-4e65-a358-15977e521198" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
The fusion of optical coherence tomography (OCT) and fundus modality information can provide a comprehensive diagnosis for retinal artery occlusion (RAO) disease, where OCT provides the cross-sectional examination of the fundus image. Given multi-modal retinal images, an anomaly diagnosis model can discriminate RAO without the need for real diseased samples. Despite this, previous studies have only focused on single-modal diagnosis, because of: 1) the lack of paired modality samples; and 2) the significant imaging differences, which make the fusion difficult with small-scale medical data. In this paper, we describe how we first built a multi-modal RAO dataset including both OCT and fundus modalities, which supports both the anomaly detection and localization tasks with pixel-level annotation. Motivated by the powerful generalization ability of the recent visual foundation model known as the Segment Anything Model (SAM), we adapted it for our task considering the small-scale property of retinal samples. Specifically, a modality-shared decoder with task-specific tokens is introduced to make SAM support the multi-modal image setting, which includes a mask token for the anomaly localization task at the pixel level and a fusion token for the anomaly detection task at the case level. Since SAM has little medical knowledge and lacks the learning of the â€œnormalâ€ concept, it is infeasible to localize RAO anomalies in the zero-shot manner. To integrate expert retinal knowledge while keeping the general segmentation knowledge, general anomaly simulation for both modalities and a low-level prompt-tuning strategy are introduced. The experiments conducted in this study show that the adapted model can surpass the state-of-the-art model by a large margin. This study sets the first benchmark for the multi-modal anomaly detection and localization tasks in the medical community. The code is available at https://github.com/Jingtao-Li-CVer/MMRAD.
</details>
  
- *[[AAAI 2025]](https://arxiv.org/pdf/2412.17297) **Revisiting Multimodal Fusion for 3D Anomaly Detection from an Architectural Perspective**

_Long ØŒ Kai Scheme DX IE ØŒ Gu O-Sample Ùˆ MA ØŒ L Ian Revet DL IU ØŒ J IA Ùˆ L U ØŒ Z HI Tide_

<div align="center">
  <img src="https://github.com/user-attachments/assets/2dd4d4f9-dbb5-4dee-b271-3deaa56ea093" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Existing efforts to boost multimodal fusion of 3D anomaly detection (3D-AD) primarily concentrate on devising more effective multimodal fusion strategies. However, little attention was devoted to analyzing the role of multimodal fusion architecture (topology) design in contributing to 3D-AD. In this paper, we aim to bridge this gap and present a systematic study on the impact of multimodal fusion architecture design on 3D-AD. This work considers the multimodal fusion architecture design at the intra-module fusion level, ie, independent modality-specific modules, involving early, middle or late multimodal features with specific fusion operations, and also at the inter-module fusion level, ie, the strategies to fuse those modules. In both cases, we first derive insights through theoretically and experimentally exploring how architectural designs influence 3D-AD. Then, we extend SOTA neural architecture search (NAS) paradigm and propose 3D-ADNAS to simultaneously search across multimodal fusion strategies and modality-specific modules for the first time. Extensive experiments show that 3D-ADNAS obtains consistent improvements in 3D-AD across various model capacities in terms of accuracy, frame rate, and memory usage, and it exhibits great potential in dealing with few-shot 3D-AD tasks.
</details>
  
  
## 10. Vision Language Models <div id = "s10"></div>

-   [\[CV PR 2025\]](https://arxiv.org/pdf/2503.06661)**AA-Clip: ØªØ¹Ø²ÙŠØ² Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„ØµÙØ± Ù…Ù† Ø®Ù„Ø§Ù„ Ù…Ù‚Ø·Ø¹ Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨Ø§Ù„Ø´Ø°ÙˆØ°**[: Octocat:](https://github.com/mwxinnn/aa-clip)

    _Ma Ùˆ Wenxin Ùˆ Zhang Ùˆ X U Andy Ùˆ Kingsong Ùˆ Tang Ùˆ Fe ng Ùˆ W ØŒ ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Chen Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ DLI ØŒ Ying Ø£ÙŠØ¶Ù‹Ø§ Ø¢Ù†Ø¯ÙŠ ØŒ Ø±Ùˆ Ø¥ÙŠØ§Ù†Ø² ØŒ Z I Ùˆ Zhou ØŒ S Kevin_

<div align="center">
  <img src="https://github.com/user-attachments/assets/86849a66-3de5-4fec-9120-57b5b98aec11" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Anomaly detection (AD) identifies outliers for applications like defect and lesion detection. While CLIP shows promise for zero-shot AD tasks due to its strong generalization capabilities, its inherent Anomaly-Unawareness leads to limited discrimination between normal and abnormal features. To address this problem, we propose Anomaly-Aware CLIP (AA-CLIP), which enhances CLIP's anomaly discrimination ability in both text and visual spaces while preserving its generalization capability. AA-CLIP is achieved through a straightforward yet effective two-stage approach: it first creates anomaly-aware text anchors to differentiate normal and abnormal semantics clearly, then aligns patch-level visual features with these anchors for precise anomaly localization. This two-stage strategy, with the help of residual adapters, gradually adapts CLIP in a controlled manner, achieving effective AD while maintaining CLIP's class knowledge. Extensive experiments validate AA-CLIP as a resource-efficient solution for zero-shot AD tasks, achieving state-of-the-art results in industrial and medical applications. The code is available at https://github.com/Mwxinnn/AA-CLIP.
</details>

-   [\[MIC CAI 2025\]](https://arxiv.org/abs/2503.01020)**Ø§Ù„Ø®ÙˆØ¶ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø®Ø§Ø±Ø¬ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ Ù„ØºØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ©**[: Octocat:](https://github.com/pyjulie/medical-vlms-ood-detection)

    _J u ØŒ Lie and Zhou ØŒ Si Nand Zhou ØŒ Y U Cool Nand L U ØŒ Huimin and Zhu ØŒ Z Huting and Keane ØŒ Pearl Sea and GE ØŒ Z on_

<div align="center">
  <img src="https://github.com/user-attachments/assets/77d7ea6b-3f7e-42ed-a02c-cf81c493b950" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Recent advances in medical vision-language models (VLMs) demonstrate impressive performance in image classification tasks, driven by their strong zero-shot generalization capabilities. However, given the high variability and complexity inherent in medical imaging data, the ability of these models to detect out-of-distribution (OOD) data in this domain remains underexplored. In this work, we conduct the first systematic investigation into the OOD detection potential of medical VLMs. We evaluate state-of-the-art VLM-based OOD detection methods across a diverse set of medical VLMs, including both general and domain-specific purposes. To accurately reflect real-world challenges, we introduce a cross-modality evaluation pipeline for benchmarking full-spectrum OOD detection, rigorously assessing model robustness against both semantic shifts and covariate shifts. Furthermore, we propose a novel hierarchical prompt-based method that significantly enhances OOD detection performance. Extensive experiments are conducted to validate the effectiveness of our approach. The codes are available at https://github.com/PyJulie/Medical-VLMs-OOD-Detection.
</details>

-   [\[Neurips 2024\]](https://proceedings.neurips.cc/paper_files/paper/2024/hash/8f4477b086a9c97e30d1a0621ea6b2f5-Abstract-Conference.html)**ÙˆØ§Ø­Ø¯ Ø¥Ù„Ù‰ ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ: ØªØ®ØµÙŠØµ Ø§Ù„Ø´Ø°ÙˆØ° Ù„Ø§ÙƒØªØ´Ø§Ù Ø´Ø°ÙˆØ° Ù‚Ù„ÙŠÙ„**

    _L I ØŒ Yi I and Zhang ØŒ S Good and L I ØŒ K'ang and Lao ØŒ Q I Cheng_

<div align="center">
  <img src="https://github.com/user-attachments/assets/ed1c9537-0238-4bb1-b7d9-df0b7148e3d7" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Traditional Anomaly Detection (AD) methods have predominantly relied on unsupervised learning from extensive normal data. Recent AD methods have evolved with the advent of large pre-trained vision-language models, enhancing few-shot anomaly detection capabilities. However, these latest AD methods still exhibit limitations in accuracy improvement. One contributing factor is their direct comparison of a query image's features with those of few-shot normal images. This direct comparison often leads to a loss of precision and complicates the extension of these techniques to more complex domainsâ€”an area that remains underexplored in a more refined and comprehensive manner. To address these limitations, we introduce the anomaly personalization method, which performs a personalized one-to-normal transformation of query images using an anomaly-free customized generation model, ensuring close alignment with the normal manifold. Moreover, to further enhance the stability and robustness of prediction results, we propose a triplet contrastive anomaly inference strategy, which incorporates a comprehensive comparison between the query and generated anomaly-free data pool and prompt information. Extensive evaluations across eleven datasets in three domains demonstrate our model's effectiveness compared to the latest AD methods. Additionally, our method has been proven to transfer flexibly to other AD methods, with the generated image data effectively improving the performance of other AD methods.
</details>

-   [\[CV PR 2024\]](http://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Toward_Generalist_Anomaly_Detection_via_In-context_Residual_Learning_with_Few-shot_CVPR_2024_paper.pdf)**Ù†Ø­Ùˆ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ø¹Ø§Ù… Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ø¹ Ù…Ø·Ø§Ù„Ø¨Ø§Øª Ø¹ÙŠÙ†Ø© Ù‚Ù„ÙŠÙ„Ø©**[: Octocat:](https://github.com/mala-lab/inctrl)

    _Z Tiger ØŒ J Ia Ùˆ Pang ØŒ Send Guan_

<div align="center">
  <img src="https://github.com/user-attachments/assets/98f8cda7-31e2-4205-a374-f57074a36e00" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
This paper explores the problem of Generalist Anomaly Detection (GAD) aiming to train one single detection model that can generalize to detect anomalies in diverse datasets from different application domains without any further training on the target data. Some recent studies have shown that large pre-trained Visual-Language Models (VLMs) like CLIP have strong generalization capabilities on detecting industrial defects from various datasets but their methods rely heavily on handcrafted text prompts about defects making them difficult to generalize to anomalies in other applications eg medical image anomalies or semantic anomalies in natural images. In this work we propose to train a GAD model with few-shot normal images as sample prompts for AD on diverse datasets on the fly. To this end we introduce a novel approach that learns an in-context residual learning model for GAD termed InCTRL. It is trained on an auxiliary dataset to discriminate anomalies from normal samples based on a holistic evaluation of the residuals between query images and few-shot normal sample prompts. Regardless of the datasets per definition of anomaly larger residuals are expected for anomalies than normal samples thereby enabling InCTRL to generalize across different domains without further training. Comprehensive experiments on nine AD datasets are performed to establish a GAD benchmark that encapsulate the detection of industrial defect anomalies medical anomalies and semantic anomalies in both one-vs-all and multi-class setting on which InCTRL is the best performer and significantly outperforms state-of-the-art competing methods. Code is available at https://github.com/mala-lab/InCTRL.
</details>
  
- [[CVPR 2024]](https://arxiv.org/abs/2403.12570) **Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images** [:octocat:](https://github.com/mediabrain-sjtu/mvfa-ad)

_Huang ØŒ C ÙØ¶ÙˆÙ„ÙŠ Nand Jiang ØŒ AO Ordurn DF Eng ØŒ Jin ÙŠØ­Ø¯Ø« Just Ùˆ Zhang ØŒ YA Ùˆ Wang ØŒ X ÙÙŠ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ùˆ Wang ØŒ Y Sarcastic_

<div align="center">
  <img src="https://github.com/user-attachments/assets/f5ed3f7b-d2cd-4291-8280-5a3c7934040f" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Recent advancements in large-scale visual-language pre-trained models have led to significant progress in zero-/few-shot anomaly detection within natural image domains. However the substantial domain divergence between natural and medical images limits the effectiveness of these methodologies in medical anomaly detection. This paper introduces a novel lightweight multi-level adaptation and comparison framework to repurpose the CLIP model for medical anomaly detection. Our approach integrates multiple residual adapters into the pre-trained visual encoder enabling a stepwise enhancement of visual features across different levels. This multi-level adaptation is guided by multi-level pixel-wise visual-language feature alignment loss functions which recalibrate the model's focus from object semantics in natural imagery to anomaly identification in medical images. The adapted features exhibit improved generalization across various medical data types even in zero-shot scenarios where the model encounters unseen medical modalities and anatomical regions during training. Our experiments on medical anomaly detection benchmarks demonstrate that our method significantly surpasses current state-of-the-art models with an average AUC improvement of 6.24% and 7.33% for anomaly classification 2.03% and 2.37% for anomaly segmentation under the zero-shot and few-shot settings respectively. Source code is available at: https://github.com/MediaBrain-SJTU/MVFA-AD.
</details>

-   [\[ICLR 2024\]](https://openreview.net/forum?id=buC4E91xZE)**Ø§Ù„Ø´Ø°ÙˆØ°: Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø³Ø±ÙŠØ¹ Ø§Ù„ÙƒØ§Ø¦Ù†-Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø³Ø±ÙŠØ¹ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ° ØµÙØ±**[: Octocat:](https://github.com/zqhang/anomalyclip)

    _Zhou ØŒ Q I Ùˆ Pang ØŒ Guan Ùˆ Ti Button ØŒ Yuan's He ØŒ Shibo Ùˆ Chen ØŒ Jim Ing_

<div align="center">
  <img src="https://github.com/user-attachments/assets/2e56974f-5ac6-489e-b2b1-75cfa910e4cf" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.
</details>

-   \*[\[ACM MM 2024\]](https://dl.acm.org/doi/abs/10.1145/3664647.3681376)**Simclip: ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø§Ø°Ø§Ø© Ù†Øµ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ Ù…Ø·Ø§Ù„Ø¨Ø§Øª Ø¨Ø³ÙŠØ·Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ø´Ø°ÙˆØ° Ø§Ù„ØµÙØ±/Ø§Ù„Ù‚Ù„ÙŠÙ„Ø©**[: Octocat:](https://github.com/CH-ORGI/SimCLIP)

    _Deng ØŒ Chengmam DX U ØŒ H Aote and Chen ØŒ Î¾ ao lu'an dx u ØŒ h audi Ùˆ tu ØŒ Î¾ aotong Ùˆ ding ØŒ xinghao Ùˆ huang ØŒ y ue_

<div align="center">
  <img src="https://github.com/user-attachments/assets/1f642822-9d76-4363-b171-ddbc7512d8eb" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Recently, large pre-trained vision-language models, such as CLIP, have demonstrated significant potential in zero-/few-shot anomaly detection tasks. However, existing methods not only rely on expert knowledge to manually craft extensive text prompts but also suffer from a misalignment of high-level language features with fine-level vision features in anomaly segmentation tasks. In this paper, we propose a method, named SimCLIP, which focuses on refining the aforementioned misalignment problem through bidirectional adaptation of both Multi-Hierarchy Vision Adapter (MHVA) and Implicit Prompt Tuning (IPT). In this way, our approach requires only a simple binary prompt to efficiently accomplish anomaly classification and segmentation tasks in zero-shot scenarios. Furthermore, we introduce its few-shot extension, SimCLIP+, integrating the relational information among vision embeddings and skillfully merging the cross-modal synergy information between vision and language to address downstream anomaly detection tasks. Extensive experiments on two challenging datasets prove the more remarkable generalization capacity of our method compared to the current SOTA approaches. Our code is available at https://github.com/CH-ORGI/SimCLIP.
</details>

-   [\[MIC CAI 2024\]](https://arxiv.org/pdf/2405.11315)**Mediclip: ØªÙƒÙŠÙŠÙ Ù…Ù‚Ø·Ø¹ Ù„Ø§ÙƒØªØ´Ø§Ù ØµÙˆØ±Ø© Ø·Ø¨ÙŠØ© Ù‚Ù„ÙŠÙ„Ø©**[: Octocat:](https://github.com/cnulab/mediclip)

    _Zhang ØŒ Î¾ Seconds Ùˆ X u ØŒ min Ùˆ q iu ØŒ d e will Andy Press ØŒ r uixin Ùˆ lang ØŒ n ing Ùˆ Zhou ØŒ Î¾ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ U_

<div align="center">
  <img src="https://github.com/user-attachments/assets/9c1c39d0-b284-4ae2-b7f9-a5a62315b16c" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
In the field of medical decision-making, precise anomaly detection in medical imaging plays a pivotal role in aiding clinicians. However, previous work is reliant on large-scale datasets for training anomaly detection models, which increases the development cost. This paper first focuses on the task of medical image anomaly detection in the few-shot setting, which is critically significant for the medical field where data collection and annotation are both very expensive. We propose an innovative approach, MediCLIP, which adapts the CLIP model to few-shot medical image anomaly detection through self-supervised fine-tuning. Although CLIP, as a vision-language model, demonstrates outstanding zero-/few-shot performance on various downstream tasks, it still falls short in the anomaly detection of medical images. To address this, we design a series of medical image anomaly synthesis tasks to simulate common disease patterns in medical imaging, transferring the powerful generalization capabilities of CLIP to the task of medical image anomaly detection. When only few-shot normal medical images are provided, MediCLIP achieves state-of-the-art performance in anomaly detection and location compared to other methods. Extensive experiments on three distinct medical anomaly detection tasks have demonstrated the superiority of our approach. The code is available at https://github.com/cnulab/MediCLIP.
</details>

-   \*[\[Neurips 2022\]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/e43a33994a28f746dcfd53eb51ed3c2d-Abstract-Conference.html)**Ø§Ù„Ø®ÙˆØ¶ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø®Ø§Ø±Ø¬ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ù…Ø¹ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù„ØºØ© Ø§Ù„Ø±Ø¤ÙŠØ©**[: Octocat:](https://github.com/deeplearning-wisc/mcm)

    _Ming ØŒ Yifei Ùˆ Cai ØŒ Ziyi Ùˆ Gu ØŒ J iu Ùˆ Sun ØŒ Yiyi Oil Ùˆ L I ØŒ Wei Ùˆ L I ØŒ Yiyi Ø§Ù„Ø¯Ø¹Ø§ÙŠØ©_

<div align="center">
  <img src="https://github.com/user-attachments/assets/b9be8e77-8441-4b77-b2f0-bff5e78e36e1" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Recognizing out-of-distribution (OOD) samples is critical for machine learning systems deployed in the open world. The vast majority of OOD detection methods are driven by a single modality (eg, either vision or language), leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of OOD detection from a single-modal to a multi-modal regime. Particularly, we propose Maximum Concept Matching (MCM), a simple yet effective zero-shot OOD detection method based on aligning visual features with textual concepts. We contribute in-depth analysis and theoretical insights to understand the effectiveness of MCM. Extensive experiments demonstrate that MCM achieves superior performance on a wide variety of real-world tasks. MCM with vision-language features outperforms a common baseline with pure visual features on a hard OOD task with semantically similar classes by 13.1% (AUROC) Code is available at https://github.com/deeplearning-wisc/MCM.
</details>

## 11. ØªÙ‚Ø·ÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙØ©<div id = "s11"></div>

-   \*[\[AIA 2025](https://arxiv.org/pdf/2412.07579)**ÙØªØ­ Ø¥Ù…ÙƒØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚Ø·ÙŠØ± Ø§Ù„Ø¹ÙƒØ³ÙŠ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø°ÙˆØ°**[: Octocat:](https://github.com/hito2448/urd)

    _L iu ØŒ Î¾new York and Wang ØŒ J Ian and Len G ØŒ Bi Ao and Zhang ØŒ S Goods_

<div align="center">
  <img src="https://github.com/user-attachments/assets/38ced3db-a3d3-4dbb-8691-308739ac5a03" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Knowledge Distillation (KD) is a promising approach for unsupervised Anomaly Detection (AD). However, the student network's over-generalization often diminishes the crucial representation differences between teacher and student in anomalous regions, leading to detection failures. To address this problem, the widely accepted Reverse Distillation (RD) paradigm designs the asymmetry teacher and student network, using an encoder as teacher and a decoder as student. Yet, the design of RD does not ensure that the teacher encoder effectively distinguishes between normal and abnormal features or that the student decoder generates anomaly-free features. Additionally, the absence of skip connections results in a loss of fine details during feature reconstruction. To address these issues, we propose RD with Expert, which introduces a novel Expert-Teacher-Student network for simultaneous distillation of both the teacher encoder and student decoder. The added expert network enhances the student's ability to generate normal features and optimizes the teacher's differentiation between normal and abnormal features, reducing missed detections. Additionally, Guided Information Injection is designed to filter and transfer features from teacher to student, improving detail reconstruction and minimizing false positives. Experiments on several benchmarks prove that our method outperforms existing unsupervised AD methods under RD paradigm, fully unlocking RDâ€™s potential.
</details>

## 12. ØªØ¹Ù„Ù… Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·<div id = "s12"></div>

-   [\[TMI 2024\]](https://ieeexplore.ieee.org/document/10680612)**ÙÙŠ Ù…ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª ÙÙŠ Ø§Ù„ØªØ´Ø§Ø¨Ù‡: Ø§Ù„ØªØ¹Ù„Ù… ØºÙŠØ± Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù Ø¯Ø§Ø®Ù„ Ø§Ù„ÙˆÙ„Ø§Ø¯Ø© ÙˆØ§Ù„ØªØ¹Ù„Ù… ØºÙŠØ± Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø´Ø°ÙˆØ° Ø§Ù„Ø£Ø´Ø¹Ø© Ø§Ù„Ø³ÙŠÙ†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ØµØ¯Ø±**

    _X U Ùˆ Shi Cheng'an DL I Ùˆ Wei Ùˆ L I Ùˆ Z Uo Ùˆ Zhao Ùˆ Tie Ùˆ Zhang ØŒ Bob_

<div align="center">
  <img src="https://github.com/user-attachments/assets/3be25c8c-d497-4def-a11c-83df2379c124" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Anomaly detection can significantly aid doctors in interpreting chest X-rays. The commonly used strategy involves utilizing the pre-trained network to extract features from normal data to establish feature representations. However, when a pre-trained network is applied to more detailed X-rays, differences of similarity can limit the robustness of these feature representations. Therefore, we propose an intra- and inter-correlation learning framework for chest X-ray anomaly detection. Firstly, to better leverage the similar anatomical structure information in chest X-rays, we introduce the Anatomical-Feature Pyramid Fusion Module for feature fusion. This module aims to obtain fusion features with both local details and global contextual information. These fusion features are initialized by a trainable feature mapper and stored in a feature bank to serve as centers for learning. Furthermore, to Facing Differences of Similarity (FDS) introduced by the pre-trained network, we propose an intra- and inter-correlation learning strategy: 1) We use intra-correlation learning to establish intra-correlation between mapped features of individual images and semantic centers, thereby initially discovering lesions; 2) We employ inter-correlation learning to establish inter-correlation between mapped features of different images, further mitigating the differences of similarity introduced by the pre-trained network, and achieving effective detection results even in diverse chest disease environments. Finally, a comparison with 18 state-of-the-art methods on three datasets demonstrates the superiority and effectiveness of the proposed method across various scenarios.
</details>

## 13. Ø¬ÙŠÙ„ Ø§Ù„Ø´Ø°ÙˆØ°<div id = "s13"></div>

-   \*[\[CV PR 2025\]](https://openaccess.thecvf.com/content/CVPR2025/html/Jin_Dual-Interrelated_Diffusion_Model_for_Few-Shot_Anomaly_Image_Generation_CVPR_2025_paper.html)**Ù†Ù…ÙˆØ°Ø¬ Ø§Ù†ØªØ´Ø§Ø± Ù…Ø²Ø¯ÙˆØ¬ Ø§Ù„Ù…Ø±ØªØ¨Ø· Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø´Ø°ÙˆØ° Ù‚Ù„ÙŠÙ„**[: Octocat:](https://github.com/yinyjin/DualAnoDiff)

    _Ø¬ÙŠÙ† ØŒ ÙŠÙŠÙ†Øº ÙˆØ¨ÙŠÙ†Ø¬ ØŒ Ø¬ÙŠÙ†Ù„ÙˆÙ†Ø¬ ØŒ Ù‡Ùˆ ØŒ ØªØ´ÙŠÙ†ØºØ¯ÙˆÙ†Øº ÙˆÙ‡Ùˆ ØŒ ØªÙŠÙ†Øº Ùˆ ÙˆÙˆ ØŒ Ø¬ÙŠÙÙˆ ÙˆØªØ´Ù† ØŒ Ù‡Ø§Ùˆ ÙˆÙˆØ§Ù†Øº ØŒ Ù‡ÙˆÙƒÙˆØ§Ù† ÙˆØªØ´Ùˆ ØŒ ÙˆÙŠÙ†Ø¨Ù†Ø¬ ÙˆØ´ÙŠ ØŒ Ù…ÙŠÙ†ØºÙ…ÙŠÙ† ÙˆÙ„ÙŠÙˆ ØŒ ÙŠÙˆÙ†ÙŠÙˆ ÙˆØ¢Ø®Ø±_

<div align="center">
  <img src="https://github.com/user-attachments/assets/1e58fa6b-99c8-4d24-9ddf-ab2215c7b4d0" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
The performance of anomaly inspection in industrial manufacturing is constrained by the scarcity of anomaly data. To overcome this challenge, researchers have started employing anomaly generation approaches to augment the anomaly dataset. However, existing anomaly generation methods suffer from limited diversity in the generated anomalies and struggle to achieve a seamless blending of this anomaly with the original image. Moreover, the generated mask is usually not aligned with the generated anomaly. In this paper, we overcome these challenges from a new perspective, simultaneously generating a pair of the overall image and the corresponding anomaly part. We propose DualAnoDiff, a novel diffusion-based few-shot anomaly image generation model, which can generate diverse and realistic anomaly images by using a dual-interrelated diffusion model, where one of them is employed to generate the whole image while the other one generates the anomaly part. Moreover, we extract background and shape information to mitigate the distortion and blurriness phenomenon in few-shot image generation. Extensive experiments demonstrate the superiority of our proposed model over state-of-the-art methods in terms of diversity, realism and the accuracy of mask. Overall, our approach significantly improves the performance of downstream anomaly inspection tasks, including anomaly detection, anomaly localization, and anomaly classification tasks. Code will be made available.
</details>

## 14. Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±<div id = "s14"></div>

-   [\[Nature Communications 2025\]](https://www.nature.com/articles/s41467-025-56321-y)**ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ù‚ÙˆÙŠ ÙÙŠ ØªØµÙˆÙŠØ± Ø§Ù„Ø¯Ù…Ø§Øº**[: Octocat:](https://github.com/compai-lab/2024-ncomms-bercea.git)

    _Bercea ØŒ Cosmin I Ùˆ Wiestler ØŒ Benedikt Ùˆ Rueckert ØŒ Ø¯Ø§Ù†ÙŠØ§Ù„ ÙˆØ´Ù†Ø§Ø¨Ù„ ØŒ Ø¬ÙˆÙ„ÙŠØ§ Ø£_

<div align="center">
  <img src="https://github.com/user-attachments/assets/03824efc-a4f8-4045-8114-8ed610015778" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Normative representation learning focuses on understanding the typical anatomical distributions from large datasets of medical scans from healthy individuals. Generative Artificial Intelligence (AI) leverages this attribute to synthesize images that accurately reflect these normative patterns. This capability enables the AI allowing them to effectively detect and correct anomalies in new, unseen pathological data without the need for expert labeling. Traditional anomaly detection methods often evaluate the anomaly detection performance, overlooking the crucial role of normative learning. In our analysis, we introduce novel metrics, specifically designed to evaluate this facet in AI models. We apply these metrics across various generative AI frameworks, including advanced diffusion models, and rigorously test them against complex and diverse brain pathologies. In addition, we conduct a large multi-reader study to compare these metrics to expertsâ€™ evaluations. Our analysis demonstrates that models proficient in normative learning exhibit exceptional versatility, adeptly detecting a wide range of unseen medical conditions. Our code is available at https://github.com/compai-lab/2024-ncomms-bercea.git.
</details>

-   [\[ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù… 2025\]](https://arxiv.org/pdf/2404.04518)**Ø§Ù„ÙˆØ³ÙŠØ·: Ø¯Ø±Ø§Ø³Ø© Ù…Ù‚Ø§Ø±Ù†Ø© Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ° ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ©**[: Octocat:](https://github.com/caiyu6666/medianomaly)

    _Cai ØŒ Zhang ØŒ Weiwen Ùˆ Chen ØŒ H Ao Ùˆ Cheng ØŒ K-Ting_

<div align="center">
  <img src="https://github.com/user-attachments/assets/59e43d96-2bfc-4bce-ab5a-282cf2c06c3c" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Anomaly detection (AD) aims at detecting abnormal samples that deviate from the expected normal patterns. Generally, it can be trained merely on normal data, without a requirement for abnormal samples, and thereby plays an important role in the recognition of rare diseases and health screening in the medical domain. Despite the emergence of numerous methods for medical AD, we observe a lack of a fair and comprehensive evaluation, which causes ambiguous conclusions and hinders the development of this field. To address this problem, this paper builds a benchmark with unified comparison. Seven medical datasets with five image modalities, including chest X-rays, brain MRIs, retinal fundus images, dermatoscopic images, and histopathology whole slide images, are curated for extensive evaluation. Thirty typical AD methods, including reconstruction and self-supervised learning-based methods, are involved in comparison of image-level anomaly classification and pixel-level anomaly segmentation. Furthermore, for the first time, we formally explore the effect of key components in existing methods, clearly revealing unresolved challenges and potential future directions. The datasets and code are available at https://github.com/caiyu6666/MedIAnomaly.
</details>

-   [\[CV PR 2024\]](https://openaccess.thecvf.com/content/CVPR2024W/VAND/html/Bao_BMAD_Benchmarks_for_Medical_Anomaly_Detection_CVPRW_2024_paper.html)**BMAD: Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ø·Ø¨ÙŠ**[: Octocat:](https://github.com/dorisbao/bmad)

    _B Grogance ØŒ Jin A Nand Sun ØŒ Han Is and Deng ØŒ Han ÙŠØ¨Ø­Ø« Ø¹Ù† He ØŒ Yin Sound Ùˆ Zhang ØŒ Z ÙŠØ¨Ø¯Ùˆ Ùˆ l i ØŒ xi_

<div align="center">
  <img src="https://github.com/user-attachments/assets/8c64c38c-2485-4c9c-ab85-9aeb3386fb9d" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Anomaly detection (AD) is a fundamental research problem in machine learning and computer vision with practical applications in industrial inspection video surveillance and medical diagnosis. In the field of medical imaging AD plays a crucial role in identifying anomalies that may indicate rare diseases or conditions. However despite its importance there is currently a lack of a universal and fair benchmark for evaluating AD methods on medical images which hinders the development of more generalized and robust AD methods in this specific domain. To address this gap we present a comprehensive evaluation benchmark for assessing AD methods on medical images. This benchmark consists of six reorganized datasets from five medical domains (ie brain MRI liver CT retinal OCT chest X-ray and digital histopathology) and three key evaluation metrics and includes a total of fifteen state-of-the-art AD algorithms. This standardized and well-curated medical benchmark with the well-structured codebase enables researchers to easily compare and evaluate different AD methods and ultimately leads to the development of more effective and robust AD algorithms for medical imaging. More information on BMAD is available in our GitHub repository: https://github.com/DorisBao/BMAD.
</details>

-   [\[Arxiv 2024\]](https://www.researchgate.net/profile/Haoyang-He-7/publication/381190391_ADer_A_Comprehensive_Benchmark_for_Multi-class_Visual_Anomaly_Detection/links/66dffcb7f84dd1716ce10dc4/ADer-A-Comprehensive-Benchmark-for-Multi-class-Visual-Anomaly-Detection.pdf)**ADer: A Comprehensive Benchmark for Multi-class Visual Anomaly Detection**[: Octocat:](https://github.com/zhangzjn/ader)

    _Zhang ØŒ Jiangning Ùˆ He ØŒ H Aoyang Ùˆ Gan ØŒ Z Yi Ye Ùˆ He ØŒ Qingdong Ùˆ Cai ØŒ y y u unlved dx ue ØŒ_

<div align="center">
  <img src="https://github.com/user-attachments/assets/56c88bcd-611f-4110-ba98-850dd4d65801" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Visual anomaly detection aims to identify anomalous regions in images through unsupervised learning paradigms, with increasing application demand and value in fields such as industrial inspection and medical lesion detection. Despite significant progress in recent years, there is a lack of comprehensive benchmarks to adequately evaluate the performance of various mainstream methods across different datasets under the practical multi-class setting. The absence of standardized experimental setups can lead to potential biases in training epochs, resolution, and metric results, resulting in erroneous conclusions. This paper addresses this issue by proposing a comprehensive visual anomaly detection benchmark, ADer, which is a modular framework that is highly extensible for new methods. The benchmark includes multiple datasets from industrial and medical domains, implementing fifteen state-of-the-art methods and nine comprehensive metrics. Additionally, we have open-sourced the GPU-assisted ADEval package to address the slow evaluation problem of metrics like time-consuming mAU-PRO on large-scale data, significantly reducing evaluation time by more than 1000-fold. Through extensive experimental results, we objectively reveal the strengths and weaknesses of different methods and provide insights into the challenges and future directions of multiclass visual anomaly detection. We hope that ADer will become a valuable resource for researchers and practitioners in the field, promoting the development of more robust and generalizable anomaly detection systems. Full codes have been attached in Appendix and open-sourced at https://github.com/zhangzjn/ader.
</details>

## ğŸ¥° ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø¬ÙˆÙ…

[![Star History Chart](https://api.star-history.com/svg?repos=diaoquesang/Paper-List-for-Medical-Anomaly-Detection&type=Date)](https://star-history.com/#diaoquesang/Paper-List-for-Medical-Anomaly-Detection&Date)

[![](https://capsule-render.vercel.app/api?type=waving&height=200&color=0:2193b0,100:45ed3F&text=Back%20to%20Top&section=footer&fontSize=30&fontAlignY=65&fontColor=FFFFFF)](#top)
